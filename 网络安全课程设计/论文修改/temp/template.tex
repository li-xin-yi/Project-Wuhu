%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
%
%
\RequirePackage{fix-cm}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{amsmath}
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Cost-sensitive Algorithm
	with Local and Global Consistency%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{}

%\titlerunning{Short form of title}        % if too long for running head

\author{Weifeng Sun       \and
        Jianli Sun %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{W. Sun \at
              School of Software, Dalian University of Technology \\
              DaLian, 116620 - China \\
              \email{}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           J. Sun \at
           School of Software, Dalian University of Technology\\
           DaLian, 116620 - China\\
           \email{sjl\_dlut@163.com}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Assuming that misclassification costs among different categories are equal, traditional graph based semi-supervised classification algorithms pursuit high classification accuracy. However, in many practical problems, misclassifying one category as another always leads to higher (or lower) cost than that in turn, such that, higher classification accuracy generally not means lower cost, which is more important in these problems. Cost-sensitive classification methods enable classifiers to pay more attention to data samples with higher cost, and then attempt to get lower cost by ensuring higher classification accuracy of the category with higher cost. In this paper, we bring cost sensitivity to local and global consistency (LGC) classifiers, and propose the CS-LGC (cost-sensitive LGC) methods, which can make better use of semi-supervised classification algorithms, and ensure high classification accuracy on the basis of reducing overall cost. At the same time, since the improved algorithm may bring some problems due to unbalanced data account, we introduce a SMOTE algorithm for further optimization. Experimental results of bank loan and diagnosis problems verify the effectiveness of CS-LGC.
\keywords{Graph based semi-supervised classification \and Cost-sensitive \and Rescale \and SMOTE}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

Recently, machine learning classification algorithms have achieved sound development in multimedia identification, health care, finance, and many other applications. Among them, Graph-based Semi-Supervised Classification based on perfect graph theory, which is relatively straightforward and easy to understand, has become one of the hotest research focuses. This method can make better use of the information from label data and a great deal of data distribution information mined from unlabeled data, then solves the problem of the less labeled data and the huge labeling costs.

High classification accuracy is the target of GSSC algorithm, which assumes that the cost of misclassification among different categories is the same. However, in many practical problems,  the costs of category classifications is different, especially in the fields of medical service, finance and network security and so on.

For example, in the bank loan problem, banks decide whether to approve the loan based on the loan applicants' personal information, which can be viewed as a binary classification problem. In the process of approving a bank loan, the cost of approving an applicant who is unable to repay the loan is much greater than that of approving an applicant who can repay the loan. Similarly, in the disease diagnosis, the cost of misdiagnose unhealthy patients as healthy is much greater than that of misdiagnose healthy patients as unhealthy. When the cost differences are inconsistent among different categories, a classifier paying more attention to the accuracy of higher cost classification problem is more practical than those treat all categories equally.
%Text with citations  and \cite{urner2011access} \cite{zhou2010semi} \cite{zhu2009introduction} \cite{budvytis2010label} \cite{zhu2003semi} \cite{zhou2004learning} \cite{gao2011active} \cite{elkan2001foundations} \cite{zhou2010multi} \cite{domingos1999metacost} \cite{khoshgoftaar2011comparing} \cite{fan1999adacost} \cite{jin2010multi} \cite{qin2008cost} \cite{liu2006influence} \cite{seiffert2008comparative} \cite{bache2013uci} \cite{germandata}

Cost-sensitive learning method \cite{gao2011active,elkan2001foundations,zhou2010multi,domingos1999metacost,fan1999adacost} takes the inconsistencies of cost among categories into account to lower the overall cost for the target. This method defines the static cost matrix to make classifiers pay more concern to the sample data with higher cost, and improves classification accuracy of higher-cost category. In the semi-supervised classification problems, labels account for a small portion of the data and most data are unlabeled. Actually, the traditional semi-supervised classification treats all categories equally, which cannot guarantee a lower overall cost. Meanwhile, cost-sensitive learning may arise less fit due to limited data set of tags, which could lead to lower classifier accuracy and weaker generalization ability. Besides, in the semi-supervised learning, we often encounter the situation called uneven data problem, in which different types of samples have different numbers of label data.

In summary, researches on cost-sensitive semi-supervised classification algorithms for unbalanced datasets are of great significance to the development of finance, medical fields, network security and many other areas.

\section{Related Work}
The effectiveness of semi-supervised learning algorithm depends on the three assumptions: manifold hypothesis\cite{urner2011access,zhou2010semi}, mlustering hypothesis and smoothness hypothesis. GSSC based on manifold hypothesis builds a figure to describe the data, as well as the relationship between the data. In this figure, nodes represent the data samples while edges with weight represent the relationships between samples. At the same time, the larger the weight is, the higher the similarity of the samples have. The process that GSSC classifiers assign labels to unlabeled data is the process of label propagation in the figure. Label Propagation Algorithm Budyytis et al. proposed in \cite{budvytis2010label}  can calculate the probability of transfer among tag data by the topology and the similarity between the samples in figure, and make a label transfer by combining node out-degree. A method of Gaussian fields and harmonic functions proposed by Zhu et al. in \cite{zhu2003semi} that make discrete prediction function slack to become continuous prediction function considers the transfer probability samples fully and have a label transfer in $k$-connection diagram. Local and global consistency, LGC proposed by Zhou et al. in the \cite{zhou2004learning} introduced clustering hypothesis and had a label transfer by using local and global consistency. LGC algorithm gives a rigorous mathematical logic derivation and proves the convergence. However, these related works never consider the problem of inconsistent classification cost.

Cost-sensitive learning method is an effective way to solve the problem of the inconsistency of costs. Cost-sensitive learning method introduces cost matrix to describe the inconsistency of costs among categories and get global minimum cost. The cost-sensitive classification methods can be divided into two categories: Rescale and Reweight, depending on the representations of cost. In the method of Rescale, differences in cost are described as differences among the numbers of samples, such as Cost-sensitive sampling\cite{gao2011active}, Rebalance\cite{elkan2001foundations} and Rescale new\cite{zhou2010multi} etc. This method constructs different sample data sets according to the difference in costs which make classifiers' decision face prefer samples with more costly category. Reweight method describes differences in costs by differences in weight among samples of different types. In the method of Reweight, samples with costly category have a higher weight, which make a greater impact to classifier. MetaCost\cite{domingos1999metacost} is a typical representative of such Reweight methods. MetaCost based on Bayesian risk theory adds the cost of the sensitive nature for Non-cost-sensitive classification algorithm by using bagging\cite{khoshgoftaar2011comparing}.

AdaBoost algorithm was proposed in\cite{jin2010multi} which offers the possibility for multiple weak classifiers aggregating into a global strong classifier. AdaCost method\cite{fan1999adacost} was proposed as a cost-sensitive classification algorithm based on AdaBoost. AdaCost is based on the Reweight at the same time, and introduces cost performance function for classifiers by heuristic strategy. AdaCost forces classifier to pay more attention to costly samples, hence it shows some advantages in cost-sensitive classification problems. However, cost performance function introduced in the theoretical analysis have not been verified and damages the most important characteristics of Boosting,  which makes the algorithm not converge to the Bayesian decision. Qin etc. in \cite{qin2008cost} did try to combine semi-supervised classification algorithms with cost-sensitive learning methods, and improved classical EM algorithm by introducing misclassification cost in the process of probability assessment.

In this paper, advantages of many unlabeled data are fully taken. We use the method of Rescale to describe cost inconsistency and introduce cost-sensitive nature for LGC algorithm classic semi-supervised classification algorithm. We propose a cost-sensitive LGC methodâ€”CS-LGC algorithm. At the same time, we take the impact caused by unbalanced data into account for CS-LGC algorithm, improve the CS-LGC algorithm by proposing the average similarity concept and introduce SMOTE algorithm. The main contribution of this paper is as follows:
\begin{enumerate}[(1)]
  \item Introduce cost-sensitive nature for LGC algorithm, and propose cost-sensitive LGC algorithm;
  \item Propose CSS-LGC algorithm. We take the impact caused by unbalanced data into account for CS-LGC algorithm, and we propose optimized CS-LGC algorithmâ€”CSS-LGC algorithm.
  \item Analyze the experimental threshold, and verify the rationality of the threshold. 
  \item Demonstrate the effectiveness of the algorithm by experiment on German Credit Data Set and Breast Cancer Data Set.
\end{enumerate}

\section{CSS-LGC Algorithm}
\subsection{Problem Definition}
Many problems can be described as binary classification problems (multiple class classification problems can be split into multiple binary classification problem), so this paper simply discusses binary classification problem. The two classes can be defined as positive class whose sample number is denoted as ${N_ + }$, and negative class with the sample number denoted as ${N_ - }$. We further assume $C_{-+}$, the cost that a sample in negative class is classified into positive class is much bigger than that in turn.${\rm{X}} = \left\{ {{x_1},{x_2}, \ldots ,{x_l},{x_{l + 1}}, \ldots ,{x_{l + u}}} \right\}$ is a set of data samples, where $l$ is the number of labeled data, and $\left\{ {{x_1},{x_2}, \ldots ,{x_l}} \right\}$ forms the initial labeled data sets. $u$ represents the number of unlabeled data, thus $U=l+u$ denotes the total number of samples. ${x_i}$ is a multidimensional vector of the $i$-th features in data, which can be viewed as a
point in higher dimensional space. Those data points and their relationships can be described by a fully connected graph $G = \left( {V,E} \right)$, where $V \in {R^{N \times P}}$ represents the vertex set of $G$, $P$ is the feature value of each data sample. we have $V = X$ here, and $E$ is the edge set of $G$. Similarity among data points is important in classification, clustering and other machine learning fields. In LGC algorithm, similarity among data points is described by weight matrix $W \in {R^{N \times N}}$. Labeled data information is described by labeled information matrix $Y \in {L^{N \times 2}}$, where $L = \left\{ { + 1, - 1} \right\}$  If data $i$ belongs to category $j$, then ${Y_{ij}} = 1$, otherwise ${Y_{ij}} = 0$. The initial labeled information matrix only has a little labeled information.

Liu and Zhou mentioned that the cost of misclassification can be standardized in \cite{liu2006influence}, which will not affect the best decision when calculation is simplified. In this paper, we let ${c_{ +  - }} = 1$, so  ${c_{ - + }}$ is a real number larger than $1$. In order to make semi-supervised classifier cost sensitive, We introduce sample cost information for the initial label matrix, denote cost difference using different initial labeled information volumes, and process the initial label matrix as Formula \ref{formula1} shows:
\begin{equation} \label{formula1}
YN_{.i} = {T_0} \circ {Y_{ \cdot i}} 
\end{equation}
${N_{.i}}$ indicates the $i$-th column of the processed initial label matrix. $\circ $ indicates Adama product. $T_0$ indicates the cost of sample data at the initial time. In addition
\begin{equation}
{T_0}({x_i})=
  \begin{cases}
  1/{N_ +}& \mbox{if } x_i \mbox{ is positive class} \\
  {C_{-+}}/{N_ - } & \mbox{if } x_i \mbox{ is negative class}  
  \end{cases}
\end{equation}

CS-LGC method is designed to classify all data samples using the information of graph $G$ with unknown $Y$ and ${T_0}$. Our goal is to ensure unlabeled data to obtain class label as much as
possible without changing the information of labeled data, and at the same time, minimize the global  classification cost without damaging classification accuracy.
\subsection{Algorithm Process}
CS-LGC method is a boosting process that trains semi-supervised classifiers in each iteration and update labeled data set based on the performance of the classifiers.
At the initial time, semi-supervised classifier $h_0$ is trained using traditional LGC algorithm
according to preprocessed initial label matrix, and we further calculate error rate by Formula \ref{formula3}.
\begin{equation} \label{formula3}
  {\varepsilon _0} = \frac{{\mathop \sum \nolimits_{i = 1}^N I\left( {{y_{i,{h_0}\left( {{x_i}} \right)}} = 0} \right)}}{N}
\end{equation}
Here $I$ is a binary function, whose value is $1$ if conditions are satisfied, or is 0 else. ${h_0}\left( {{x_i}} \right)$ shows classification results for $x_i$ at the initial time. Then according to the AdaBoost algorithm, cost information of misclassified samples increases while cost information of correctly classified samples decreases following Formula \ref{formula4},
\begin{equation} \label{formula4}
  {T_{t + 1}}\left( {{x_i}} \right) = \frac{{{T_t}\left( {{x_i}} \right)}}{{{Z_t}}} \times 
  \begin{cases}
    {{e^{ - {\alpha_t}}}} & \text{if } {{y_{i,{h_t}\left( {{x_i}} \right)}} = 1} \\
    {{e^{{\alpha_t}}}} & \text{if } {{y_{i,{h_t}\left( {{x_i}} \right)}} = 0}
  \end{cases}
\end{equation}
where ${T_{t + 1}}$ represents the updated cost matrix, and
\begin{equation}
  {Z_t} = \mathop \sum \limits_{i = 1}^N {T_t}\left( {{x_i}} \right) \times 
  \begin{cases}
     {{e^{ - {\alpha_t}}}} & \text{if } {{y_{i,{h_t}\left( {{x_i}} \right)}} = 1} \\
     {{e^{{\alpha_t}}}} & \text{if } {{y_{i,{h_t}\left( {{x_i}} \right)}} = 0}   
  \end{cases}
\end{equation}
where ${\alpha _t}$ represents the weight of classifier $h_t$í±¡ in the final global classification at time $t$, and can be calculated by
\begin{equation}
  {\alpha _t} = \ln \left( {\frac{{1 - {\varepsilon _t}}}{{{\varepsilon _t}}}} \right)
\end{equation}

The main objective of Cost-sensitive classifier is to minimize global classification. To avoid the
impact of data set scale on the results, the calculation of global cost in our model use the mean cost method brought by Seiffert et al\cite{seiffert2008comparative}. Since we have to standardize classification cost of our model, the corresponding mean cost needs to be updated according to Formula \ref{formula7}:
\begin{equation} \label{formula7}
  PEC=\frac{{\# {f_{pos}} + \# {f_{neg}} \cdot {C_{ -  + }}}}{{\# {t_{pos}} + \# {t_{neg}} + \# {f_{pos}} + \# {f_{neg}}}}
\end{equation}
where $PEC$ denotes the average cost of classifiers, ${\# {t_{pos}}}$ and ${\# {f_{pos}}}$  denote the number of positive class samples classified correctly and classified incorrectly respectively. ${\# {t_{neg}}}$ and ${\# {f_{neg}}}$ represent the corresponding information of negative class. The global cost we refer to here that is indeed the average cost. From Formula \ref{formula7} we can see that the correct classified samples does not increase the global cost.
% \section{Section title}
% \label{sec:1}
%Text with citations  and %\cite{urner2011access} \cite{zhou2010semi} \cite{zhu2009introduction} \cite{budvytis2010label} \cite{zhu2003semi} \cite{zhou2004learning} \cite{gao2011active} \cite{elkan2001foundations} \cite{zhou2010multi} \cite{domingos1999metacost} \cite{khoshgoftaar2011comparing} \cite{fan1999adacost} \cite{jin2010multi} \cite{qin2008cost} \cite{liu2006influence} \cite{seiffert2008comparative} \cite{bache2013uci} \cite{germandata}
% \subsection{Subsection title}
% \label{sec:2}
% as required. Don't forget to give each section
% and subsection a unique label (see Sect.~\ref{sec:1}).graphystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base
\bibliographystyle{ieeetr}
\bibliography{myref}

% Non-BibTeX users please use
\end{document}
% end of file template.tex

